import torch
import torch.nn as nn
import torch.nn.functional as F
from .FSGNet import *  # å¯¼å…¥æ‰€æœ‰FSGNetç»„ä»¶


class RegionGrowingModule(nn.Module):
    """å¯å¾®åˆ†çš„åŒºåŸŸç”Ÿé•¿æ¨¡å—ï¼Œé›†æˆåˆ°ä¸Šé‡‡æ ·è¿‡ç¨‹ä¸­"""
    
    def __init__(self, in_channels, mu_f=0.0789, sigma_f=0.0774, alpha=1.0):
        super(RegionGrowingModule, self).__init__()
        
        # åŒºåŸŸç”Ÿé•¿å‚æ•°
        self.register_parameter('mu_f', nn.Parameter(torch.tensor(mu_f, dtype=torch.float32)))
        self.register_parameter('sigma_f', nn.Parameter(torch.tensor(sigma_f, dtype=torch.float32)))
        self.register_parameter('alpha', nn.Parameter(torch.tensor(alpha, dtype=torch.float32)))
        
        # Sobelå·ç§¯æ ¸
        sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32).unsqueeze(0).unsqueeze(0)
        sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32).unsqueeze(0).unsqueeze(0)
        self.register_buffer('sobel_x', sobel_x)
        self.register_buffer('sobel_y', sobel_y)
        
        # è†¨èƒ€æ ¸
        self.register_buffer('dilation_kernel', torch.ones(1, 1, 3, 3, dtype=torch.float32))
        
        # ç‰¹å¾å¢å¼ºå·ç§¯
        self.enhance_conv = nn.Sequential(
            nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(in_channels),
            nn.ReLU(inplace=True)
        )
        
        # èåˆæƒé‡
        self.fusion_weight = nn.Parameter(torch.tensor(0.7))
    
    def extract_green_channel(self, x):
        """ä»ç‰¹å¾å›¾ä¸­æå–ç±»ä¼¼ç»¿è‰²é€šé“çš„ä¿¡æ¯"""
        if x.shape[1] == 3:  # å¦‚æœæ˜¯RGBå›¾åƒ
            return x[:, 1:2, :, :]  # ç›´æ¥å–ç»¿è‰²é€šé“
        else:  # å¦‚æœæ˜¯ç‰¹å¾å›¾
            return torch.mean(x, dim=1, keepdim=True)
    
    def calculate_gradient(self, x):
        """è®¡ç®—æ¢¯åº¦å¹…å€¼"""
        green_like = self.extract_green_channel(x)
        grad_x = F.conv2d(green_like, self.sobel_x, padding=1)
        grad_y = F.conv2d(green_like, self.sobel_y, padding=1)
        gradient_magnitude = torch.sqrt(grad_x ** 2 + grad_y ** 2 + 1e-8)
        return gradient_magnitude
    
    def soft_region_growing(self, features, gradient_magnitude):
        """è½¯åŒºåŸŸç”Ÿé•¿"""
        B, C, H, W = features.shape
        
        # è®¡ç®—è¡€ç®¡é˜ˆå€¼
        vessel_threshold = self.mu_f + self.alpha * self.sigma_f
        vessel_candidates = torch.sigmoid((gradient_magnitude - vessel_threshold) * 10.0)
        
        # ä½¿ç”¨ç‰¹å¾å›¾çš„æ¿€æ´»ä½œä¸ºç§å­
        feature_activation = torch.mean(features, dim=1, keepdim=True)
        seed_threshold = feature_activation.mean(dim=[2, 3], keepdim=True) + \
                        0.5 * feature_activation.std(dim=[2, 3], keepdim=True)
        seeds = torch.sigmoid((feature_activation - seed_threshold) * 10.0)
        
        # è¿­ä»£è½¯è†¨èƒ€
        grown_mask = seeds
        for _ in range(2):
            dilated = F.conv2d(grown_mask, self.dilation_kernel, padding=1)
            dilated = torch.clamp(dilated, 0, 1)
            new_grown = dilated * vessel_candidates
            grown_mask = torch.max(grown_mask, new_grown * 0.5)
        
        return grown_mask
    
    def forward(self, features, original_image=None):
        """
        Args:
            features: [B, C, H, W] ä¸Šé‡‡æ ·çš„ç‰¹å¾å›¾
            original_image: [B, 3, H', W'] åŸå§‹è¾“å…¥å›¾åƒï¼ˆå¯é€‰ï¼‰
        Returns:
            enhanced_features: [B, C, H, W] å¢å¼ºåçš„ç‰¹å¾å›¾
        """
        # å¢å¼ºç‰¹å¾
        enhanced = self.enhance_conv(features)
        
        # è®¡ç®—æ¢¯åº¦
        if original_image is not None:
            upsampled_image = F.interpolate(original_image, size=features.shape[2:], mode='bilinear', align_corners=True)
            gradient_magnitude = self.calculate_gradient(upsampled_image)
        else:
            gradient_magnitude = self.calculate_gradient(features)
        
        # è½¯åŒºåŸŸç”Ÿé•¿
        grown_mask = self.soft_region_growing(enhanced, gradient_magnitude)
        
        # æ‰©å±•åˆ°æ‰€æœ‰é€šé“å¹¶èåˆ
        grown_mask_expanded = grown_mask.expand(-1, features.shape[1], -1, -1)
        fusion_weight = torch.sigmoid(self.fusion_weight)
        final_features = fusion_weight * enhanced + (1 - fusion_weight) * (enhanced * grown_mask_expanded)
        
        return final_features


class RG_FSGNet(nn.Module):
    """é›†æˆåŒºåŸŸç”Ÿé•¿çš„FSGNet - åœ¨ä¸Šé‡‡æ ·è¿‡ç¨‹ä¸­åº”ç”¨RG"""
    
    def __init__(self, channel, n_classes, base_c, depths, kernel_size, 
                 enable_rg=True, mu_f=0.0789, sigma_f=0.0774, alpha=1.0):
        super(RG_FSGNet, self).__init__()  # ğŸ”¥ ä¿®å¤ï¼šç±»åè¦ä¸€è‡´ï¼

        self.enable_rg = enable_rg
        
        # å¤åˆ¶åŸå§‹FSGNetçš„æ‰€æœ‰ç»„ä»¶
        self.input_layer = nn.Sequential(
            M_Conv(channel, base_c * 1, kernel_size=kernel_size),
            *[ConvNext(base_c * 1, kernel_size=kernel_size) for _ in range(depths[0])]
        )
        self.input_skip = nn.Sequential(
            M_Conv(channel, base_c * 1, kernel_size=kernel_size),
        )
        self.conv1 = M_Conv(channel, base_c * 1, kernel_size=3)

        self.down_conv_2 = nn.Sequential(*[
            nn.Conv2d(base_c * 2, base_c * 2, kernel_size=2, stride=2),
            *[ConvNext(base_c * 2, kernel_size=kernel_size) for _ in range(depths[1])]
            ])
        self.conv2 = M_Conv(channel, base_c * 2, kernel_size=3)

        self.down_conv_3 = nn.Sequential(*[
            nn.Conv2d(base_c * 4, base_c * 4, kernel_size=2, stride=2),
            *[ConvNext(base_c * 4, kernel_size=kernel_size) for _ in range(depths[2])]
            ])
        self.conv3 = M_Conv(channel, base_c * 4, kernel_size=3)

        self.down_conv_4 = nn.Sequential(*[
            nn.Conv2d(base_c * 8, base_c * 8, kernel_size=2, stride=2),
            *[ConvNext(base_c * 8, kernel_size=kernel_size) for _ in range(depths[3])]
            ])
        self.attn = SelfAttentionBlock()

        self.up_residual_conv3 = ResidualConv(base_c * 8, base_c * 4, 1, 1)
        self.up_residual_conv2 = ResidualConv(base_c * 4, base_c * 2, 1, 1)
        self.up_residual_conv1 = ResidualConv(base_c * 2, base_c * 1, 1, 1)

        # ã€æ–°å¢ã€‘åŒºåŸŸç”Ÿé•¿æ¨¡å—åˆ°ä¸Šé‡‡æ ·è·¯å¾„
        if self.enable_rg:
            self.rg_module_3 = RegionGrowingModule(base_c * 4, mu_f, sigma_f, alpha)
            self.rg_module_2 = RegionGrowingModule(base_c * 2, mu_f, sigma_f, alpha)
            self.rg_module_1 = RegionGrowingModule(base_c * 1, mu_f, sigma_f, alpha)

        self.output_layer3 = nn.Sequential(
            nn.Conv2d(base_c * 4, n_classes, 1, 1),
            nn.Sigmoid(),
        )
        self.output_layer2 = nn.Sequential(
            nn.Conv2d(base_c * 2, n_classes, 1, 1),
            nn.Sigmoid(),
        )
        self.output_layer1 = nn.Sequential(
            nn.Conv2d(base_c * 1, n_classes, 1, 1),
            nn.Sigmoid(),
        )

        self.fgf = FastGuidedFilter_attention(r=2, eps=1e-2)
        self.attention_block3 = CrossAttentionBlock(in_channels=base_c * 8)
        self.attention_block2 = CrossAttentionBlock(in_channels=base_c * 4)
        self.attention_block1 = CrossAttentionBlock(in_channels=base_c * 2)

        self.conv_cat_3 = M_Conv(base_c * 8 + base_c * 8, base_c * 8, kernel_size=1)
        self.conv_cat_2 = M_Conv(base_c * 8 + base_c * 4, base_c * 4, kernel_size=1)
        self.conv_cat_1 = M_Conv(base_c * 4 + base_c * 2, base_c * 2, kernel_size=1)

    def forward(self, x):
        # ä¿å­˜åŸå§‹è¾“å…¥ç”¨äºåŒºåŸŸç”Ÿé•¿
        original_input = x
        
        # Get multi-scale from input
        _, _, h, w = x.size()
        x_scale_2 = F.interpolate(x, size=(h // 2, w // 2), mode='bilinear', align_corners=True)
        x_scale_3 = F.interpolate(x, size=(h // 4, w // 4), mode='bilinear', align_corners=True)

        # Encoder
        x1 = self.input_layer(x) + self.input_skip(x)
        x1_conv = self.conv1(x)
        x1_down = torch.cat([x1_conv, x1], dim=1)

        x2 = self.down_conv_2(x1_down)
        x2_conv = self.conv2(x_scale_2)
        x2_down = torch.cat([x2_conv, x2], dim=1)

        x3 = self.down_conv_3(x2_down)
        x3_conv = self.conv3(x_scale_3)
        x3_down = torch.cat([x3_conv, x3], dim=1)

        x4 = self.down_conv_4(x3_down)
        x4 = self.attn(x4)

        # Decoder with integrated region growing
        # ç¬¬ä¸€è½®ä¸Šé‡‡æ ·
        _, _, h, w = x3_down.size()
        x3_gf = torch.cat([x3_down, F.interpolate(x4, size=(h, w), mode='bilinear', align_corners=True)], dim=1)
        x3_gf_conv = self.conv_cat_3(x3_gf)
        x3_small = F.interpolate(x3_gf_conv, size=(h // 2, w // 2), mode='bilinear', align_corners=True)
        fgf_out = self.fgf(x3_small, x4, x3_gf_conv, self.attention_block3(x3_small, x4))
        x3_up = self.up_residual_conv3(fgf_out)
        
        # ğŸ”¥ ç¬¬ä¸€æ¬¡åŒºåŸŸç”Ÿé•¿ï¼šåœ¨ç¬¬ä¸€è½®ä¸Šé‡‡æ ·ååº”ç”¨
        if self.enable_rg:
            x3_up = self.rg_module_3(x3_up, original_input)

        # ç¬¬äºŒè½®ä¸Šé‡‡æ ·
        _, _, h, w = x2_down.size()
        x2_gf = torch.cat([x2_down, F.interpolate(x3_gf_conv, size=(h, w), mode='bilinear', align_corners=True)], dim=1)
        x2_gf_conv = self.conv_cat_2(x2_gf)
        x2_small = F.interpolate(x2_gf_conv, size=(h // 2, w // 2), mode='bilinear', align_corners=True)
        fgf_out = self.fgf(x2_small, x3_up, x2_gf_conv, self.attention_block2(x2_small, x3_up))
        x2_up = self.up_residual_conv2(fgf_out)
        
        # ğŸ”¥ ç¬¬äºŒæ¬¡åŒºåŸŸç”Ÿé•¿ï¼šåœ¨ç¬¬äºŒè½®ä¸Šé‡‡æ ·ååº”ç”¨ï¼ˆæ¢å¤ï¼ï¼‰
        if self.enable_rg:
            x2_up = self.rg_module_2(x2_up, original_input)

        # ç¬¬ä¸‰è½®ä¸Šé‡‡æ ·
        _, _, h, w = x1_down.size()
        x1_gf = torch.cat([x1_down, F.interpolate(x2_gf_conv, size=(h, w), mode='bilinear', align_corners=True)], dim=1)
        x1_gf_conv = self.conv_cat_1(x1_gf)
        x1_small = F.interpolate(x1_gf_conv, size=(h // 2, w // 2), mode='bilinear', align_corners=True)
        fgf_out = self.fgf(x1_small, x2_up, x1_gf_conv, self.attention_block1(x1_small, x2_up))
        x1_up = self.up_residual_conv1(fgf_out)
        
        # ğŸ”¥ ç¬¬ä¸‰æ¬¡åŒºåŸŸç”Ÿé•¿ï¼šåœ¨ç¬¬ä¸‰è½®ä¸Šé‡‡æ ·ååº”ç”¨ï¼ˆæ¢å¤ï¼ï¼‰
        if self.enable_rg:
            x1_up = self.rg_module_1(x1_up, original_input)

        # è¾“å‡ºå±‚
        _, _, h, w = original_input.size()
        out_3 = F.interpolate(x3_up, size=(h, w), mode='bilinear', align_corners=True)
        out_2 = F.interpolate(x2_up, size=(h, w), mode='bilinear', align_corners=True)
        out_3 = self.output_layer3(out_3)
        out_2 = self.output_layer2(out_2)
        out_1 = self.output_layer1(x1_up)

        return out_1, out_2, out_3

