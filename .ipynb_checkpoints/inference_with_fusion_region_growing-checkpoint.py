import torch
import torch.nn as nn
import cv2
import numpy as np
from models.backbones.FSGNet import FSGNet
from models import dataloader as dataloader_hub
import argparse
from PIL import Image
import os


class SimpleRegionGrowing:
    """ç®€å•åŒºåŸŸç”Ÿé•¿ç±»"""
    def __init__(self, mu_f=0.0789, sigma_f=0.0774, alpha=1.0):
        self.mu_f = mu_f
        self.sigma_f = sigma_f  
        self.alpha = alpha
        self.vessel_threshold = mu_f + alpha * sigma_f
    
    def preprocess_image(self, rgb_image):
        """é¢„å¤„ç†å•å¼ å›¾åƒ"""
        # rgb_image: [3, H, W] tensor
        green_channel = rgb_image[1].cpu().numpy()  # [H, W]
        
        # å½’ä¸€åŒ–
        min_val, max_val = green_channel.min(), green_channel.max()
        if max_val > min_val:
            normalized = (green_channel - min_val) / (max_val - min_val)
        else:
            normalized = green_channel
        
        # èƒŒæ™¯å‡é™¤å¢å¼º
        background = cv2.GaussianBlur(normalized, (101, 101), 0)
        enhanced = np.clip(normalized - background, 0, 1)
        
        return enhanced
    
    def calculate_gradient(self, enhanced_image):
        """è®¡ç®—æ¢¯åº¦"""
        # Sobelç®—å­
        grad_x = cv2.Sobel(enhanced_image, cv2.CV_64F, 1, 0, ksize=3)
        grad_y = cv2.Sobel(enhanced_image, cv2.CV_64F, 0, 1, ksize=3)
        gradient_magnitude = np.sqrt(grad_x**2 + grad_y**2)
        
        return gradient_magnitude
    
    def region_growing(self, fsg_prediction, gradient_magnitude, fsg_threshold):
        """åŒºåŸŸç”Ÿé•¿"""
        # fsg_prediction: [H, W] numpy array (0-1æ¦‚ç‡)
        
        # 1. å°†FSGé¢„æµ‹æ¦‚ç‡è½¬æ¢ä¸ºäºŒå€¼ç§å­ç‚¹
        seed_binary = (fsg_prediction > fsg_threshold).astype(np.uint8)
        print(f"Initial seed points: {seed_binary.sum()} pixels")
        
        if seed_binary.sum() == 0:
            print("No seed points found!")
            return np.zeros_like(fsg_prediction, dtype=np.float32)
        
        # 2. åŸºäºæ¢¯åº¦é˜ˆå€¼é€‰æ‹©å€™é€‰è¡€ç®¡åƒç´ 
        vessel_candidates = (gradient_magnitude >= self.vessel_threshold).astype(np.uint8)
        print(f"Vessel candidates (gradient >= {self.vessel_threshold:.4f}): {vessel_candidates.sum()} pixels")
        
        # 3. åŒºåŸŸç”Ÿé•¿ï¼šä»ç§å­ç‚¹æ‰©å±•åˆ°å€™é€‰åƒç´ 
        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
        grown_mask = seed_binary.copy()
        
        # è¿­ä»£æ‰©å±•ï¼šåªåœ¨å€™é€‰åƒç´ åŒºåŸŸå†…æ‰©å±•
        for i in range(5):
            # è†¨èƒ€å½“å‰æ©ç 
            dilated = cv2.dilate(grown_mask, kernel, iterations=1)
            # åªä¿ç•™åœ¨è¡€ç®¡å€™é€‰åŒºåŸŸå†…çš„æ‰©å±•
            new_grown = dilated * vessel_candidates
            
            # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ–°å¢é•¿
            if np.array_equal(new_grown, grown_mask):
                print(f"Converged after {i+1} iterations")
                break
            grown_mask = new_grown
        
        print(f"Final grown region: {grown_mask.sum()} pixels")
        return grown_mask.astype(np.float32)
    
    def create_circular_mask(self, image_shape):
        """åˆ›å»ºåœ†å½¢æ©ç ï¼Œå»é™¤è¾¹æ¡†åŒºåŸŸ"""
        h, w = image_shape
        center_y, center_x = h // 2, w // 2
        radius = min(center_y, center_x) * 0.9  # ç¨å¾®å°ä¸€ç‚¹é¿å…è¾¹ç¼˜
        
        Y, X = np.ogrid[:h, :w]
        mask = ((X - center_x) ** 2 + (Y - center_y) ** 2) <= radius ** 2
        return mask.astype(np.float32)
    
    def get_dynamic_threshold(self, prediction, circular_mask):
        """è®¡ç®—åŠ¨æ€é˜ˆå€¼"""
        valid_pixels = prediction[circular_mask > 0]
        if len(valid_pixels) > 0:
            pred_mean = valid_pixels.mean()
            pred_std = valid_pixels.std()
            # ä½¿ç”¨å‡å€¼ + 0.5 * æ ‡å‡†å·®ä½œä¸ºé˜ˆå€¼
            threshold = pred_mean + 0.5 * pred_std
            print(f"Dynamic threshold: {threshold:.3f} (mean={pred_mean:.3f}, std={pred_std:.3f})")
        else:
            threshold = 0.5
            print(f"Using default threshold: {threshold:.3f}")
        return threshold
    
    def fuse_multi_outputs(self, out1, out2, out3, circular_mask):
        """ä½¿ç”¨é˜ˆå€¼åˆ¤æ–­åçš„å¹¶é›†èåˆå¤šä¸ªè¾“å‡º"""
        print("=== Multi-Output Fusion ===")
        
        # å¯¹æ¯ä¸ªè¾“å‡ºè®¡ç®—åŠ¨æ€é˜ˆå€¼
        out1_masked = out1 * circular_mask
        out2_masked = out2 * circular_mask
        out3_masked = out3 * circular_mask
        
        threshold1 = self.get_dynamic_threshold(out1_masked, circular_mask)
        threshold2 = self.get_dynamic_threshold(out2_masked, circular_mask)
        threshold3 = self.get_dynamic_threshold(out3_masked, circular_mask)
        
        # äºŒå€¼åŒ–æ¯ä¸ªè¾“å‡º
        binary1 = (out1_masked > threshold1).astype(np.float32)
        binary2 = (out2_masked > threshold2).astype(np.float32)
        binary3 = (out3_masked > threshold3).astype(np.float32)
        
        print(f"Binary pixels - out1: {binary1.sum():.0f}, out2: {binary2.sum():.0f}, out3: {binary3.sum():.0f}")
        
        # è®¡ç®—å¹¶é›†
        fused_binary = np.logical_or(np.logical_or(binary1, binary2), binary3).astype(np.float32)
        
        # åº”ç”¨åœ†å½¢æ©ç 
        fused_result = fused_binary * circular_mask
        
        print(f"Fused binary pixels: {fused_result.sum():.0f}")
        print(f"Coverage increase vs out1: {fused_result.sum() - binary1.sum():.0f} pixels")
        
        return fused_result, (threshold1, threshold2, threshold3), (binary1, binary2, binary3)
    
    def refine_prediction(self, fsg_output, original_image):
        """ç»†åŒ–FSGé¢„æµ‹ç»“æœ"""
        # fsg_output: [H, W] numpy array (FSGé¢„æµ‹æ¦‚ç‡ 0-1)
        # original_image: [3, H, W] tensor
        
        print(f"FSG prediction range: [{fsg_output.min():.3f}, {fsg_output.max():.3f}]")
        
        # åˆ›å»ºåœ†å½¢æ©ç å»é™¤è¾¹æ¡†
        circular_mask = self.create_circular_mask(fsg_output.shape)
        fsg_output_masked = fsg_output * circular_mask
        print(f"After circular masking: [{fsg_output_masked.min():.3f}, {fsg_output_masked.max():.3f}]")
        
        # 1. é¢„å¤„ç†å›¾åƒ
        enhanced = self.preprocess_image(original_image)
        enhanced_masked = enhanced * circular_mask  # åŒæ ·åº”ç”¨æ©ç 
        
        # 2. è®¡ç®—æ¢¯åº¦
        gradient = self.calculate_gradient(enhanced_masked)
        print(f"Gradient range: [{gradient.min():.3f}, {gradient.max():.3f}]")
        
        # 3. è®¡ç®—åŠ¨æ€é˜ˆå€¼
        fsg_threshold = self.get_dynamic_threshold(fsg_output_masked, circular_mask)
        
        # 4. åŒºåŸŸç”Ÿé•¿ï¼šä½¿ç”¨åŠ¨æ€é˜ˆå€¼çš„FSGé¢„æµ‹ä½œä¸ºç§å­ç‚¹
        grown_result = self.region_growing(fsg_output_masked, gradient, fsg_threshold)
        
        # 5. èåˆç­–ç•¥ï¼šå–å¹¶é›†ï¼ˆORæ“ä½œï¼‰
        fsg_binary = (fsg_output_masked > fsg_threshold).astype(np.float32)
        final_result = np.logical_or(fsg_binary, grown_result).astype(np.float32)
        
        # åº”ç”¨åœ†å½¢æ©ç åˆ°æœ€ç»ˆç»“æœ
        final_result = final_result * circular_mask
        
        print(f"FSG binary pixels: {fsg_binary.sum():.0f}")
        print(f"Region grown pixels: {grown_result.sum():.0f}")
        print(f"Final result pixels: {final_result.sum():.0f}")
        print(f"Net gain: {final_result.sum() - fsg_binary.sum():.0f} pixels")
        
        return final_result


def create_overlay_image(original_image, prediction, ground_truth=None, colors=None):
    """åˆ›å»ºå åŠ å¯è§†åŒ–å›¾åƒ"""
    if colors is None:
        colors = {
            'prediction': [0, 1, 0],      # ç»¿è‰² - é¢„æµ‹
            'ground_truth': [1, 0, 0],    # çº¢è‰² - çœŸå®æ ‡ç­¾
            'both': [1, 1, 0],            # é»„è‰² - é‡å åŒºåŸŸ
            'alpha': 0.6                  # é€æ˜åº¦
        }
    
    # ç¡®ä¿è¾“å…¥æ˜¯numpyæ•°ç»„
    if isinstance(original_image, torch.Tensor):
        original_image = original_image.cpu().numpy()
    if isinstance(prediction, torch.Tensor):
        prediction = prediction.cpu().numpy()
    if ground_truth is not None and isinstance(ground_truth, torch.Tensor):
        ground_truth = ground_truth.cpu().numpy()
    
    # è½¬æ¢ä¸ºRGBæ ¼å¼ [H, W, 3]
    if original_image.ndim == 3 and original_image.shape[0] == 3:
        original_image = original_image.transpose(1, 2, 0)
    
    # åˆ›å»ºå åŠ å›¾åƒ
    overlay = original_image.copy()
    
    # äºŒå€¼åŒ–é¢„æµ‹ç»“æœ
    pred_binary = prediction > 0.5
    
    if ground_truth is not None:
        # äºŒå€¼åŒ–çœŸå®æ ‡ç­¾
        gt_binary = ground_truth > 0.5
        
        # åˆ›å»ºä¸åŒåŒºåŸŸçš„æ©ç 
        both_mask = pred_binary & gt_binary      # ä¸¤è€…éƒ½æœ‰ - é»„è‰²
        pred_only = pred_binary & (~gt_binary)   # åªæœ‰é¢„æµ‹ - ç»¿è‰²
        gt_only = gt_binary & (~pred_binary)     # åªæœ‰çœŸå®æ ‡ç­¾ - çº¢è‰²
        
        # åº”ç”¨é¢œè‰²
        alpha = colors['alpha']
        
        # é»„è‰²åŒºåŸŸ (é‡å )
        overlay[both_mask] = (1 - alpha) * overlay[both_mask] + alpha * np.array(colors['both'])
        
        # ç»¿è‰²åŒºåŸŸ (ä»…é¢„æµ‹)
        overlay[pred_only] = (1 - alpha) * overlay[pred_only] + alpha * np.array(colors['prediction'])
        
        # çº¢è‰²åŒºåŸŸ (ä»…çœŸå®æ ‡ç­¾)
        overlay[gt_only] = (1 - alpha) * overlay[gt_only] + alpha * np.array(colors['ground_truth'])
        
    else:
        # åªæœ‰é¢„æµ‹ç»“æœ
        alpha = colors['alpha']
        overlay[pred_binary] = (1 - alpha) * overlay[pred_binary] + alpha * np.array(colors['prediction'])
    
    return np.clip(overlay, 0, 1)


def create_refinement_improvement_overlay(original_image, fsg_prediction, refined_prediction, ground_truth):
    """åˆ›å»ºæ˜¾ç¤ºç»†åŒ–æ”¹è¿›æ•ˆæœçš„å åŠ å›¾"""
    # ç¡®ä¿è¾“å…¥æ˜¯numpyæ•°ç»„
    if isinstance(original_image, torch.Tensor):
        original_image = original_image.cpu().numpy()
    if isinstance(fsg_prediction, torch.Tensor):
        fsg_prediction = fsg_prediction.cpu().numpy()
    if isinstance(refined_prediction, torch.Tensor):
        refined_prediction = refined_prediction.cpu().numpy()
    if isinstance(ground_truth, torch.Tensor):
        ground_truth = ground_truth.cpu().numpy()
    
    # è½¬æ¢ä¸ºRGBæ ¼å¼
    if original_image.ndim == 3 and original_image.shape[0] == 3:
        original_image = original_image.transpose(1, 2, 0)
    
    # äºŒå€¼åŒ–
    fsg_binary = fsg_prediction > 0.5
    refined_binary = refined_prediction > 0.5
    gt_binary = ground_truth > 0.5
    
    # è®¡ç®—å˜åŒ–åŒºåŸŸ
    added_pixels = refined_binary & (~fsg_binary)     # ç»†åŒ–æ–°å¢çš„åƒç´ 
    removed_pixels = fsg_binary & (~refined_binary)   # ç»†åŒ–ç§»é™¤çš„åƒç´ 
    
    # åˆ†ææ–°å¢åƒç´ çš„æ­£ç¡®æ€§
    added_correct = added_pixels & gt_binary          # æ–°å¢çš„æ­£ç¡®åƒç´ ï¼ˆçœŸæ­£çš„æ”¹è¿›ï¼‰
    added_incorrect = added_pixels & (~gt_binary)     # æ–°å¢çš„é”™è¯¯åƒç´ ï¼ˆå‡é˜³æ€§å¢åŠ ï¼‰
    
    # åˆ†æç§»é™¤åƒç´ çš„å½±å“
    removed_correct = removed_pixels & gt_binary      # ç§»é™¤çš„æ­£ç¡®åƒç´ ï¼ˆçœŸé˜³æ€§ä¸¢å¤±ï¼‰
    removed_incorrect = removed_pixels & (~gt_binary) # ç§»é™¤çš„é”™è¯¯åƒç´ ï¼ˆå‡é˜³æ€§å‡å°‘ï¼Œå¥½äº‹ï¼‰
    
    # åˆ›å»ºå åŠ å›¾åƒ
    overlay = original_image.copy()
    alpha = 0.7
    
    # åº”ç”¨é¢œè‰²ç¼–ç 
    overlay[added_correct] = (1 - alpha) * overlay[added_correct] + alpha * np.array([0, 1, 0])      # ç»¿è‰² - æ­£ç¡®æ”¹è¿›
    overlay[added_incorrect] = (1 - alpha) * overlay[added_incorrect] + alpha * np.array([1, 0, 0])  # çº¢è‰² - é”™è¯¯å¢åŠ 
    overlay[removed_correct] = (1 - alpha) * overlay[removed_correct] + alpha * np.array([0, 0, 1])  # è“è‰² - æ­£ç¡®ä¸¢å¤±
    overlay[removed_incorrect] = (1 - alpha) * overlay[removed_incorrect] + alpha * np.array([1, 1, 0]) # é»„è‰² - é”™è¯¯å‡å°‘
    
    # ç»Ÿè®¡ä¿¡æ¯
    improvement_stats = {
        'added_correct': int(added_correct.sum()),
        'added_incorrect': int(added_incorrect.sum()),
        'removed_correct': int(removed_correct.sum()),
        'removed_incorrect': int(removed_incorrect.sum()),
        'net_improvement': int(added_correct.sum() - removed_correct.sum()),
        'net_false_positive_change': int(added_incorrect.sum() - removed_incorrect.sum())
    }
    
    return np.clip(overlay, 0, 1), improvement_stats


def create_fusion_comparison_overlay(original_image, fused_prediction, individual_outputs, ground_truth=None):
    """åˆ›å»ºèåˆç»“æœä¸å•ä¸ªè¾“å‡ºçš„å¯¹æ¯”å›¾"""
    # individual_outputs: (out1, out2, out3) tuple
    out1, out2, out3 = individual_outputs
    
    # ç¡®ä¿è¾“å…¥æ˜¯numpyæ•°ç»„
    if isinstance(original_image, torch.Tensor):
        original_image = original_image.cpu().numpy()
    if isinstance(fused_prediction, torch.Tensor):
        fused_prediction = fused_prediction.cpu().numpy()
    
    # è½¬æ¢ä¸ºRGBæ ¼å¼
    if original_image.ndim == 3 and original_image.shape[0] == 3:
        original_image = original_image.transpose(1, 2, 0)
    
    # åˆ›å»ºå åŠ å›¾åƒ
    overlay = original_image.copy()
    
    # äºŒå€¼åŒ–
    fused_binary = fused_prediction > 0.5
    out1_binary = out1 > 0.5
    out2_binary = out2 > 0.5
    out3_binary = out3 > 0.5
    
    # åˆ›å»ºä¸åŒåŒºåŸŸçš„æ©ç 
    fusion_only = fused_binary & (~(out1_binary | out2_binary | out3_binary))  # ä»…èåˆæœ‰çš„ - çº¢è‰²
    common_area = fused_binary & out1_binary  # èåˆä¸out1å…±åŒåŒºåŸŸ - ç»¿è‰²
    out2_contrib = fused_binary & out2_binary & (~out1_binary)  # out2è´¡çŒ® - è“è‰²
    out3_contrib = fused_binary & out3_binary & (~out1_binary) & (~out2_binary)  # out3è´¡çŒ® - ç´«è‰²
    
    # åº”ç”¨é¢œè‰²
    alpha = 0.6
    overlay[common_area] = (1 - alpha) * overlay[common_area] + alpha * np.array([0, 1, 0])      # ç»¿è‰²
    overlay[out2_contrib] = (1 - alpha) * overlay[out2_contrib] + alpha * np.array([0, 0, 1])    # è“è‰²
    overlay[out3_contrib] = (1 - alpha) * overlay[out3_contrib] + alpha * np.array([1, 0, 1])    # ç´«è‰²
    overlay[fusion_only] = (1 - alpha) * overlay[fusion_only] + alpha * np.array([1, 0, 0])      # çº¢è‰²
    
    return np.clip(overlay, 0, 1)


def load_fsg_model(model_path, device):
    """åŠ è½½FSGæ¨¡å‹"""
    model = FSGNet(channel=3, n_classes=1, base_c=64, depths=[3, 3, 9, 3], kernel_size=3)
    
    # åŠ è½½æƒé‡
    checkpoint = torch.load(model_path, map_location='cpu')
    
    # å¤„ç†DataParallelä¿å­˜çš„æƒé‡ (å»æ‰module.FSGNet.å‰ç¼€)
    state_dict = {}
    for key, value in checkpoint.items():
        if key.startswith('module.FSGNet.'):
            # å»æ‰ 'module.FSGNet.' å‰ç¼€
            new_key = key[len('module.FSGNet.'):]
            state_dict[new_key] = value
        elif key.startswith('module.'):
            # å»æ‰ 'module.' å‰ç¼€
            new_key = key[len('module.'):]
            state_dict[new_key] = value
        else:
            state_dict[key] = value
    
    model.load_state_dict(state_dict)
    model.to(device)
    model.eval()
    print(f"Successfully loaded FSG model with {len(state_dict)} parameters")
    return model


def find_ground_truth(img_file, possible_gt_dirs):
    """æŸ¥æ‰¾å¯¹åº”çš„çœŸå®æ ‡ç­¾æ–‡ä»¶"""
    base_name = os.path.splitext(img_file)[0]
    
    # å¸¸è§çš„æ ‡ç­¾æ–‡ä»¶å‘½åæ¨¡å¼
    possible_names = [
        f"{base_name}_manual1.gif",
        f"{base_name}_manual1.png",
        f"{base_name}_manual1.tif",
        f"{base_name}_gt.png",
        f"{base_name}_gt.gif",
        f"{base_name}_gt.tif",
        f"{base_name}.png",
        f"{base_name}.gif",
        f"{base_name}.tif"
    ]
    
    for gt_dir in possible_gt_dirs:
        if os.path.exists(gt_dir):
            for possible_name in possible_names:
                gt_path = os.path.join(gt_dir, possible_name)
                if os.path.exists(gt_path):
                    return gt_path
    return None


def inference_with_fusion_region_growing():
    # é…ç½® - å…ˆå°è¯•valç›®å½•ï¼Œå¦‚æœä¸ºç©ºåˆ™ä½¿ç”¨trainç›®å½•
    fsg_model_path = '/root/FSG-Net-pytorch/model_ckpts/FSG-Net-DRIVE.pt'
    
    # å°è¯•å¤šä¸ªå¯èƒ½çš„æ•°æ®è·¯å¾„
    possible_paths = [
        '/root/FSG-Net-pytorch/data/DRIVE/val/input',
        '/root/FSG-Net-pytorch/data/DRIVE/train/input',
        '/root/FSG-Net-pytorch/data/DRIVE/val',
        '/root/FSG-Net-pytorch/data/DRIVE/train'
    ]
    
    # å¯èƒ½çš„çœŸå®æ ‡ç­¾è·¯å¾„
    possible_gt_dirs = [
        '/root/FSG-Net-pytorch/data/DRIVE/val/gt',
        '/root/FSG-Net-pytorch/data/DRIVE/train/gt',
        '/root/FSG-Net-pytorch/data/DRIVE/val/label',
        '/root/FSG-Net-pytorch/data/DRIVE/train/label',
        '/root/FSG-Net-pytorch/data/DRIVE/val/mask',
        '/root/FSG-Net-pytorch/data/DRIVE/train/mask'
    ]
    
    test_data_path = None
    for path in possible_paths:
        if os.path.exists(path):
            files = [f for f in os.listdir(path) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.tif', '.tiff', '.bmp'))]
            if len(files) > 0:
                test_data_path = path
                print(f"Found {len(files)} images in: {path}")
                break
            else:
                print(f"Path exists but no images found: {path}")
        else:
            print(f"Path does not exist: {path}")
    
    if test_data_path is None:
        print("No valid image directory found!")
        print("Please check your data structure or put some test images in one of these directories:")
        for path in possible_paths:
            print(f"  {path}")
        return
    
    # æ›´æ¢è¾“å‡ºç›®å½•
    output_dir = './fusion_region_growing_results'
    
    # åŒºåŸŸç”Ÿé•¿å‚æ•°
    mu_f = 0.0789
    sigma_f = 0.0774
    alpha = 1.0
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")
    
    # åŠ è½½FSGæ¨¡å‹
    print("Loading FSG-Net model...")
    fsg_model = load_fsg_model(fsg_model_path, device)
    
    # åˆå§‹åŒ–åŒºåŸŸç”Ÿé•¿
    region_grower = SimpleRegionGrowing(mu_f, sigma_f, alpha)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    os.makedirs(f"{output_dir}/fsg_individual", exist_ok=True)  # å•ä¸ªè¾“å‡º
    os.makedirs(f"{output_dir}/fsg_fusion", exist_ok=True)      # èåˆè¾“å‡º
    os.makedirs(f"{output_dir}/refined_individual", exist_ok=True)  # å•ä¸ªè¾“å‡ºç»†åŒ–
    os.makedirs(f"{output_dir}/refined_fusion", exist_ok=True)      # èåˆè¾“å‡ºç»†åŒ–
    os.makedirs(f"{output_dir}/overlay_fusion_vs_gt", exist_ok=True)        # èåˆç»“æœä¸GTå¯¹æ¯”
    os.makedirs(f"{output_dir}/overlay_fusion_comparison", exist_ok=True)   # èåˆä¸å•ä¸ªè¾“å‡ºå¯¹æ¯”
    os.makedirs(f"{output_dir}/improvement_individual", exist_ok=True)      # å•ä¸ªè¾“å‡ºæ”¹è¿›
    os.makedirs(f"{output_dir}/improvement_fusion", exist_ok=True)          # èåˆè¾“å‡ºæ”¹è¿›
    
    # æŸ¥æ‰¾å›¾åƒæ–‡ä»¶
    all_files = os.listdir(test_data_path)
    image_files = [f for f in all_files if f.lower().endswith(('.png', '.jpg', '.jpeg', '.tif', '.tiff', '.bmp'))]
    
    print(f"Found {len(all_files)} total files in {test_data_path}")
    print(f"Found {len(image_files)} image files")
    
    if len(image_files) == 0:
        print("No image files found! Listing all files in directory:")
        for f in all_files[:10]:  # æ˜¾ç¤ºå‰10ä¸ªæ–‡ä»¶
            print(f"  {f}")
        return
    
    print(f"Processing {len(image_files)} images...")
    
    # å¤„ç†æ¯å¼ å›¾åƒ
    for i, img_file in enumerate(image_files):
        print(f"Processing {i+1}/{len(image_files)}: {img_file}")
        print("=" * 80)
        
        try:
            # åŠ è½½å›¾åƒ
            img_path = os.path.join(test_data_path, img_file)
            image = Image.open(img_path).convert('RGB')
            print(f"  Image size: {image.size}")
            
            # è½¬æ¢ä¸ºtensorå¹¶å½’ä¸€åŒ–
            img_array = np.array(image) / 255.0
            img_tensor = torch.tensor(img_array.transpose(2, 0, 1), dtype=torch.float32).unsqueeze(0).to(device)
            print(f"  Tensor shape: {img_tensor.shape}")
            
            # æŸ¥æ‰¾å¯¹åº”çš„çœŸå®æ ‡ç­¾
            gt_path = find_ground_truth(img_file, possible_gt_dirs)
            ground_truth = None
            if gt_path:
                try:
                    gt_image = Image.open(gt_path).convert('L')
                    ground_truth = np.array(gt_image) / 255.0
                    print(f"  Found ground truth: {os.path.basename(gt_path)}")
                except Exception as e:
                    print(f"  Error loading ground truth: {e}")
                    ground_truth = None
            else:
                print(f"  No ground truth found for {img_file}")
            
            # FSGæ¨ç† - è·å–ä¸‰ä¸ªè¾“å‡º
            with torch.no_grad():
                fsg_out1, fsg_out2, fsg_out3 = fsg_model(img_tensor)
                
                # è½¬æ¢ä¸ºnumpy
                out1 = fsg_out1.squeeze().cpu().numpy()  # [H, W]
                out2 = fsg_out2.squeeze().cpu().numpy()  # [H, W]
                out3 = fsg_out3.squeeze().cpu().numpy()  # [H, W]
                
                print(f"  Output shapes - out1: {out1.shape}, out2: {out2.shape}, out3: {out3.shape}")
                print(f"  Output ranges - out1: [{out1.min():.3f}, {out1.max():.3f}]")
                print(f"  Output ranges - out2: [{out2.min():.3f}, {out2.max():.3f}]")
                print(f"  Output ranges - out3: [{out3.min():.3f}, {out3.max():.3f}]")
            
            # åˆ›å»ºåœ†å½¢æ©ç 
            circular_mask = region_grower.create_circular_mask(out1.shape)
            
            # å¤šè¾“å‡ºèåˆï¼ˆé˜ˆå€¼åˆ¤æ–­åçš„å¹¶é›†ï¼‰
            fused_result, thresholds, individual_binaries = region_grower.fuse_multi_outputs(
                out1, out2, out3, circular_mask
            )
            
            # åŒºåŸŸç”Ÿé•¿ç»†åŒ–
            original_tensor = img_tensor.squeeze()  # [3, H, W]
            
            # å¯¹å•ä¸ªè¾“å‡ºè¿›è¡Œç»†åŒ–
            print("\n=== Individual Output Refinement ===")
            refined_out1 = region_grower.refine_prediction(out1, original_tensor)
            
            # å¯¹èåˆè¾“å‡ºè¿›è¡Œç»†åŒ–
            print("\n=== Fusion Output Refinement ===")
            refined_fusion = region_grower.refine_prediction(fused_result, original_tensor)
            
            # ä¿å­˜ç»“æœ
            base_name = os.path.splitext(img_file)[0]
            
            # 1. ä¿å­˜å•ä¸ªFSGè¾“å‡º
            out1_img = (out1 * 255).astype(np.uint8)
            out2_img = (out2 * 255).astype(np.uint8)
            out3_img = (out3 * 255).astype(np.uint8)
            Image.fromarray(out1_img).save(f"{output_dir}/fsg_individual/{base_name}_out1.png")
            Image.fromarray(out2_img).save(f"{output_dir}/fsg_individual/{base_name}_out2.png")
            Image.fromarray(out3_img).save(f"{output_dir}/fsg_individual/{base_name}_out3.png")
            
            # 2. ä¿å­˜èåˆè¾“å‡º
            fused_img = (fused_result * 255).astype(np.uint8)
            Image.fromarray(fused_img).save(f"{output_dir}/fsg_fusion/{base_name}_fused.png")
            
            # 3. ä¿å­˜ç»†åŒ–ç»“æœ
            refined_out1_img = (refined_out1 * 255).astype(np.uint8)
            refined_fusion_img = (refined_fusion * 255).astype(np.uint8)
            Image.fromarray(refined_out1_img).save(f"{output_dir}/refined_individual/{base_name}_refined_out1.png")
            Image.fromarray(refined_fusion_img).save(f"{output_dir}/refined_fusion/{base_name}_refined_fusion.png")
            
            # 4. åˆ›å»ºå’Œä¿å­˜overlayå›¾åƒ
            if ground_truth is not None:
                # èåˆç»“æœä¸çœŸå®æ ‡ç­¾çš„overlay
                fusion_gt_overlay = create_overlay_image(img_array, refined_fusion, ground_truth)
                fusion_gt_overlay_img = (fusion_gt_overlay * 255).astype(np.uint8)
                Image.fromarray(fusion_gt_overlay_img).save(f"{output_dir}/overlay_fusion_vs_gt/{base_name}_fusion_vs_gt.png")
                
                # å•ä¸ªè¾“å‡ºæ”¹è¿›æ•ˆæœå›¾
                individual_improvement_overlay, individual_stats = create_refinement_improvement_overlay(
                    img_array, out1, refined_out1, ground_truth
                )
                individual_improvement_img = (individual_improvement_overlay * 255).astype(np.uint8)
                Image.fromarray(individual_improvement_img).save(f"{output_dir}/improvement_individual/{base_name}_individual_improvement.png")
                
                # èåˆè¾“å‡ºæ”¹è¿›æ•ˆæœå›¾
                fusion_improvement_overlay, fusion_stats = create_refinement_improvement_overlay(
                    img_array, fused_result, refined_fusion, ground_truth
                )
                fusion_improvement_img = (fusion_improvement_overlay * 255).astype(np.uint8)
                Image.fromarray(fusion_improvement_img).save(f"{output_dir}/improvement_fusion/{base_name}_fusion_improvement.png")
                
                # æ‰“å°æ”¹è¿›ç»Ÿè®¡ä¿¡æ¯
                print(f"\n=== Individual Output (out1) Improvement Stats ===")
                print(f"  âœ… æ–°å¢æ­£ç¡®åƒç´ : {individual_stats['added_correct']}")
                print(f"  âŒ æ–°å¢é”™è¯¯åƒç´ : {individual_stats['added_incorrect']}")
                print(f"  âš ï¸  ä¸¢å¤±æ­£ç¡®åƒç´ : {individual_stats['removed_correct']}")
                print(f"  âœ… å‡å°‘é”™è¯¯åƒç´ : {individual_stats['removed_incorrect']}")
                print(f"  ğŸ“ˆ å‡€æ”¹è¿›(æ­£ç¡®åƒç´ ): {individual_stats['net_improvement']}")
                print(f"  ğŸ“Š å‡é˜³æ€§å‡€å˜åŒ–: {individual_stats['net_false_positive_change']}")
                
                print(f"\n=== Fusion Output Improvement Stats ===")
                print(f"  âœ… æ–°å¢æ­£ç¡®åƒç´ : {fusion_stats['added_correct']}")
                print(f"  âŒ æ–°å¢é”™è¯¯åƒç´ : {fusion_stats['added_incorrect']}")
                print(f"  âš ï¸  ä¸¢å¤±æ­£ç¡®åƒç´ : {fusion_stats['removed_correct']}")
                print(f"  âœ… å‡å°‘é”™è¯¯åƒç´ : {fusion_stats['removed_incorrect']}")
                print(f"  ğŸ“ˆ å‡€æ”¹è¿›(æ­£ç¡®åƒç´ ): {fusion_stats['net_improvement']}")
                print(f"  ğŸ“Š å‡é˜³æ€§å‡€å˜åŒ–: {fusion_stats['net_false_positive_change']}")
                
                # å¯¹æ¯”ä¸ªäººè¾“å‡ºvsèåˆè¾“å‡ºçš„æ”¹è¿›æ•ˆæœ
                print(f"\n=== Individual vs Fusion Comparison ===")
                print(f"  Individualå‡€æ”¹è¿›: {individual_stats['net_improvement']}")
                print(f"  Fusionå‡€æ”¹è¿›: {fusion_stats['net_improvement']}")
                improvement_diff = fusion_stats['net_improvement'] - individual_stats['net_improvement']
                print(f"  Fusionç›¸å¯¹ä¼˜åŠ¿: {improvement_diff:+d} pixels")
            
            # 5. èåˆä¸å•ä¸ªè¾“å‡ºçš„å¯¹æ¯”
            fusion_comparison_overlay = create_fusion_comparison_overlay(
                img_array, refined_fusion, (refined_out1, out2, out3), ground_truth
            )
            fusion_comparison_img = (fusion_comparison_overlay * 255).astype(np.uint8)
            Image.fromarray(fusion_comparison_img).save(f"{output_dir}/overlay_fusion_comparison/{base_name}_fusion_comparison.png")
            
            print(f"\n  Results saved for {base_name}")
            print("=" * 80)
            
        except Exception as e:
            print(f"  Error processing {img_file}: {str(e)}")
            continue
    
    print(f"\nAll results saved to {output_dir}")
    print(f"Check the following directories:")
    print(f"  - {output_dir}/fsg_individual/ (å•ä¸ªFSGè¾“å‡º: out1, out2, out3)")
    print(f"  - {output_dir}/fsg_fusion/ (é˜ˆå€¼å¹¶é›†èåˆç»“æœ)")
    print(f"  - {output_dir}/refined_individual/ (å•ä¸ªè¾“å‡ºç»†åŒ–ç»“æœ)")
    print(f"  - {output_dir}/refined_fusion/ (èåˆè¾“å‡ºç»†åŒ–ç»“æœ)")
    print(f"  - {output_dir}/overlay_fusion_vs_gt/ (èåˆç»†åŒ–ç»“æœä¸çœŸå®æ ‡ç­¾å¯¹æ¯”)")
    print(f"  - {output_dir}/overlay_fusion_comparison/ (èåˆä¸å•ä¸ªè¾“å‡ºå¯¹æ¯”)")
    print(f"  - {output_dir}/improvement_individual/ (å•ä¸ªè¾“å‡ºæ”¹è¿›æ•ˆæœå›¾ â­)")
    print(f"  - {output_dir}/improvement_fusion/ (èåˆè¾“å‡ºæ”¹è¿›æ•ˆæœå›¾ â­)")
    
    print("\nèåˆç­–ç•¥è¯´æ˜:")
    print("  ğŸ” å¯¹æ¯ä¸ªè¾“å‡º(out1, out2, out3)åˆ†åˆ«è®¡ç®—åŠ¨æ€é˜ˆå€¼")
    print("  ğŸ¯ å°†æ¯ä¸ªè¾“å‡ºäºŒå€¼åŒ–åå–å¹¶é›†(Union)")
    print("  ğŸ“ˆ èåˆç»“æœåŒ…å«äº†æ‰€æœ‰è¾“å‡ºçš„è¡€ç®¡ä¿¡æ¯")
    print("  ğŸ”¬ åŒºåŸŸç”Ÿé•¿è¿›ä¸€æ­¥ç»†åŒ–èåˆç»“æœ")
    
    print("\nOverlayé¢œè‰²è¯´æ˜:")
    print("  æ”¹è¿›æ•ˆæœå›¾:")
    print("    ğŸŸ¢ ç»¿è‰² = ç»†åŒ–æ–°å¢çš„æ­£ç¡®åƒç´  (çœŸæ­£çš„æ”¹è¿›!)")
    print("    ğŸ”´ çº¢è‰² = ç»†åŒ–æ–°å¢çš„é”™è¯¯åƒç´  (å‡é˜³æ€§å¢åŠ )")
    print("    ğŸ”µ è“è‰² = ç»†åŒ–ä¸¢å¤±çš„æ­£ç¡®åƒç´  (çœŸé˜³æ€§ä¸¢å¤±)")
    print("    ğŸŸ¡ é»„è‰² = ç»†åŒ–å‡å°‘çš„é”™è¯¯åƒç´  (å‡é˜³æ€§å‡å°‘,å¥½äº‹)")
    print("  èåˆå¯¹æ¯”å›¾:")
    print("    ğŸŸ¢ ç»¿è‰² = èåˆä¸out1å…±åŒåŒºåŸŸ")
    print("    ğŸ”µ è“è‰² = out2ç‹¬æœ‰è´¡çŒ®")
    print("    ğŸŸ£ ç´«è‰² = out3ç‹¬æœ‰è´¡çŒ®")
    print("    ğŸ”´ çº¢è‰² = ä»…èåˆæœ‰çš„åŒºåŸŸ")


if __name__ == "__main__":
    inference_with_fusion_region_growing()